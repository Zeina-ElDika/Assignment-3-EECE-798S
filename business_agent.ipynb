{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8445e8e3",
      "metadata": {
        "id": "8445e8e3"
      },
      "source": [
        "# GiftMuse Atelier Concierge - Gemini Colab App\n",
        "\n",
        "Everything required to demo the GiftMuse Atelier gifting concierge in Google Colab lives in this notebook. It bundles a Gemini-powered agent, lead & feedback logging, sample interactions, and a colorful Gradio chat UI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e4a68a6",
      "metadata": {
        "id": "0e4a68a6"
      },
      "source": [
        "## Quick Start\n",
        "1. Place your `.env` file alongside this notebook (already populated per your request).\n",
        "2. Open the notebook in Colab.\n",
        "3. (Optional) Run the install cell if the required packages are missing.\n",
        "4. Execute the remaining cells sequentially to chat with the concierge and view logged leads/feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a61ae5a8",
      "metadata": {
        "id": "a61ae5a8"
      },
      "outputs": [],
      "source": [
        "# Optional: install dependencies if your environment does not already have them.\n",
        "# Uncomment if needed.\n",
        "!pip install -q google-generativeai gradio PyPDF2 python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ced6c93",
      "metadata": {
        "id": "1ced6c93"
      },
      "source": [
        "## Load Gemini API Key from `.env`\n",
        "Your key is loaded securely via `python-dotenv` so it is not hard-coded in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for key in [\"HTTP_PROXY\", \"HTTPS_PROXY\", \"http_proxy\", \"https_proxy\"]:\n",
        "    os.environ.pop(key, None)\n"
      ],
      "metadata": {
        "id": "igI08jajuasS"
      },
      "id": "igI08jajuasS",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ee99920c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ee99920c",
        "outputId": "2493b4f2-6fea-45e8-c260-77c61c452814"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyCjiNPQvwZmTSRLMOL4KSvqvjHAtpJIFtg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError('GEMINI_API_KEY is missing. Add it to your .env file before proceeding.')\n",
        "GEMINI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "print(genai.GenerativeModel(\"models/gemini-2.5-flash\").generate_content(\"Hello!\").text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "RBS-ZUqKubYL",
        "outputId": "f35601bf-3954-4d4e-f4fc-4a6612c7f871"
      },
      "id": "RBS-ZUqKubYL",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello there! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d31d0d3",
      "metadata": {
        "id": "0d31d0d3"
      },
      "source": [
        "## Preferred Gemini Models\n",
        "The agent will default to `models/gemini-2.5-flash` and automatically fall back to additional options if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d44b72a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d44b72a3",
        "outputId": "605dbc49-8e3a-413f-d74d-cb05ef72a20e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['models/gemini-2.5-flash',\n",
              " 'models/gemini-1.5-flash-latest',\n",
              " 'models/gemini-1.5-flash',\n",
              " 'models/gemini-1.5-flash-8b-latest',\n",
              " 'models/gemini-1.5-pro-latest',\n",
              " 'models/gemini-1.5-pro']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "MODEL_CANDIDATES = [\n",
        "    'models/gemini-2.5-flash',\n",
        "    'models/gemini-1.5-flash-latest',\n",
        "    'models/gemini-1.5-flash',\n",
        "    'models/gemini-1.5-flash-8b-latest',\n",
        "    'models/gemini-1.5-pro-latest',\n",
        "    'models/gemini-1.5-pro',\n",
        "]\n",
        "MODEL_CANDIDATES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e9238260",
      "metadata": {
        "id": "e9238260"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "from google.api_core import exceptions as google_exceptions\n",
        "\n",
        "# --- Business content -------------------------------------------------------\n",
        "summary_text = \"GiftMuse Atelier is a gifting intelligence studio that blends warm human curation with agile AI research. We deliver quick, heartfelt gift ideas that feel tailor-made for every person and milestone.\\n\\nWe focus on concierge gifting paths: on-demand scouting sprints, event gifting programs, and the GiftGlow Corporate service for sales and HR teams. Clients trust our 120 artisan maker partners and sustainability pledge donating one percent of concierge packages to creativity grants.\"\n",
        "profile_snippet = \"GiftMuse Atelier Business Snapshot\\nMission: Replace gifting stress with confidence through curated, sentimental suggestions.\\nServices: Gift scouting sprints, signature event gifting programs, GiftGlow Corporate concierge.\\nTeam: Ava Moreno (Founder), Idris Patel (CTO), Priya Das (Customer Journey Lead).\\nProcess: Blend human curiosity with AI taste modeling to spotlight artisan and sustainable vendors.\\nValue: Collective of 120 makers, sustainability pledge, wrap-and-delivery partners.\\nTone: Warm, organized, proactive about transforming occasions into memories.\"\n",
        "\n",
        "# --- Data directory for logs ------------------------------------------------\n",
        "DATA_DIR = Path(\"giftmuse_data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "LEADS_LOG = DATA_DIR / \"leads.jsonl\"\n",
        "FEEDBACK_LOG = DATA_DIR / \"feedback.jsonl\"\n",
        "\n",
        "# --- Tool helpers -----------------------------------------------------------\n",
        "def _timestamp() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "\n",
        "def _write_log(path: Path, payload: Dict[str, str]) -> None:\n",
        "    with path.open(\"a\", encoding=\"utf-8\") as handle:\n",
        "        handle.write(json.dumps(payload, ensure_ascii=False))\n",
        "        handle.write(\"\\n\")\n",
        "\n",
        "\n",
        "def record_customer_interest(email: str = \"\", name: str = \"\", message: Optional[str] = None) -> str:\n",
        "    entry = {\n",
        "        \"type\": \"lead\",\n",
        "        \"timestamp\": _timestamp(),\n",
        "        \"name\": (name or \"\").strip(),\n",
        "        \"email\": (email or \"\").strip().lower(),\n",
        "        \"message\": (message or \"\").strip(),\n",
        "    }\n",
        "    _write_log(LEADS_LOG, entry)\n",
        "    if entry[\"name\"] or entry[\"email\"]:\n",
        "        return \"Thanks! I captured your details so our concierge can reach out soon.\"\n",
        "    return \"Appreciate the interest. I logged the note for our concierge team.\"\n",
        "\n",
        "\n",
        "def record_feedback(question: str) -> str:\n",
        "    entry = {\n",
        "        \"type\": \"feedback\",\n",
        "        \"timestamp\": _timestamp(),\n",
        "        \"question\": (question or \"\").strip(),\n",
        "    }\n",
        "    _write_log(FEEDBACK_LOG, entry)\n",
        "    return \"I saved that question for the GiftMuse Atelier team so we can follow up promptly.\"\n",
        "\n",
        "\n",
        "GEMINI_FUNCTION_DECLARATIONS = [\n",
        "    {\n",
        "        \"name\": \"record_customer_interest\",\n",
        "        \"description\": \"Capture a lead's contact details and notes for concierge follow-up.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"email\": {\"type\": \"string\", \"description\": \"Customer email address, if supplied.\"},\n",
        "                \"name\": {\"type\": \"string\", \"description\": \"Customer name or representative.\"},\n",
        "                \"message\": {\"type\": \"string\", \"description\": \"Any context about their gifting needs or requests.\"},\n",
        "            },\n",
        "            \"required\": [],\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"record_feedback\",\n",
        "        \"description\": \"Log unanswered questions or feedback for the GiftMuse Atelier team.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"question\": {\"type\": \"string\", \"description\": \"The feedback item or question we could not answer.\"},\n",
        "            },\n",
        "            \"required\": [\"question\"],\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "# --- Agent utilities -------------------------------------------------------\n",
        "def build_system_prompt(summary: str, profile: str) -> str:\n",
        "    return (\n",
        "        \"You are the concierge for GiftMuse Atelier, a joyful gifting intelligence studio. \"\n",
        "        \"Reply with upbeat, organized guidance grounded in the business profile. \"\n",
        "        \"Suggest creative gift paths quickly, highlight artisan or sustainable makers, and invite clients to share contact details. \"\n",
        "        \"If you lack information, apologize briefly and call the feedback tool so the team can follow up.\\n\\n\"\n",
        "        f\"Business summary:\\n{summary}\\n\\nSnapshot from the detailed profile:\\n{profile}\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "class GiftMuseAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        api_key: str,\n",
        "        model_candidates: Optional[List[str]] = None,\n",
        "        system_prompt: Optional[str] = None,\n",
        "    ) -> None:\n",
        "        if not api_key:\n",
        "            raise ValueError(\"Gemini API key is required.\")\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        defaults = [\n",
        "            \"models/gemini-2.5-flash\",\n",
        "            \"models/gemini-1.5-flash-latest\",\n",
        "            \"models/gemini-1.5-flash\",\n",
        "            \"models/gemini-1.5-flash-8b-latest\",\n",
        "            \"models/gemini-1.5-pro-latest\",\n",
        "            \"models/gemini-1.5-pro\",\n",
        "        ]\n",
        "        supplied = model_candidates or []\n",
        "        self.model_candidates: List[str] = []\n",
        "        for candidate in supplied + defaults:\n",
        "            if candidate and candidate not in self.model_candidates:\n",
        "                self.model_candidates.append(candidate)\n",
        "\n",
        "        self.system_prompt = system_prompt or build_system_prompt(summary_text, profile_snippet)\n",
        "        self._model = None\n",
        "        self._chat = None\n",
        "        self._active_index = 0\n",
        "        self._init_chat(start_index=0)\n",
        "\n",
        "    def _init_chat(self, start_index: int) -> None:\n",
        "        last_error: Optional[Exception] = None\n",
        "        for idx in range(start_index, len(self.model_candidates)):\n",
        "            candidate = self.model_candidates[idx]\n",
        "            try:\n",
        "                model = genai.GenerativeModel(\n",
        "                    model_name=candidate,\n",
        "                    system_instruction=self.system_prompt,\n",
        "                    tools=[{\"function_declarations\": GEMINI_FUNCTION_DECLARATIONS}],\n",
        "                )\n",
        "                chat = model.start_chat(history=[])\n",
        "                self._model = model\n",
        "                self._chat = chat\n",
        "                self._active_index = idx\n",
        "                self.model_name = candidate\n",
        "                if idx != start_index:\n",
        "                    print(f\"Switched to Gemini model: {candidate}\")\n",
        "                else:\n",
        "                    print(f\"Using Gemini model: {candidate}\")\n",
        "                return\n",
        "            except (\n",
        "                google_exceptions.NotFound,\n",
        "                google_exceptions.FailedPrecondition,\n",
        "                google_exceptions.PermissionDenied,\n",
        "            ) as exc:\n",
        "                last_error = exc\n",
        "                continue\n",
        "\n",
        "        raise RuntimeError(\n",
        "            \"Unable to initialize a Gemini model. \"\n",
        "            f\"Tried: {', '.join(self.model_candidates)}. Last error: {last_error}\"\n",
        "        )\n",
        "\n",
        "    def _rotate_model(self) -> None:\n",
        "        next_index = self._active_index + 1\n",
        "        if next_index >= len(self.model_candidates):\n",
        "            raise RuntimeError(\n",
        "                \"All configured Gemini models returned errors. \"\n",
        "                f\"Models tried: {', '.join(self.model_candidates)}\"\n",
        "            )\n",
        "        self._init_chat(start_index=next_index)\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_text(response) -> str:\n",
        "        for candidate in response.candidates:\n",
        "            parts = []\n",
        "            for part in candidate.content.parts:\n",
        "                text = getattr(part, \"text\", None)\n",
        "                if text:\n",
        "                    parts.append(text)\n",
        "            if parts:\n",
        "                return \"\\n\".join(parts).strip()\n",
        "        return \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _next_tool_call(response) -> Optional[Dict[str, object]]:\n",
        "        for candidate in response.candidates:\n",
        "            for part in candidate.content.parts:\n",
        "                call = getattr(part, \"function_call\", None)\n",
        "                if call:\n",
        "                    return {\"name\": call.name, \"args\": dict(call.args or {})}\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _invoke_tool(name: str, arguments: Dict[str, object]) -> Dict[str, object]:\n",
        "        if name == \"record_customer_interest\":\n",
        "            result_text = record_customer_interest(\n",
        "                email=arguments.get(\"email\", \"\"),\n",
        "                name=arguments.get(\"name\", \"\"),\n",
        "                message=arguments.get(\"message\"),\n",
        "            )\n",
        "        elif name == \"record_feedback\":\n",
        "            result_text = record_feedback(question=arguments.get(\"question\", \"\"))\n",
        "        else:\n",
        "            result_text = \"Tool not implemented.\"\n",
        "        return {\n",
        "            \"function_response\": {\n",
        "                \"name\": name,\n",
        "                \"response\": {\"text\": result_text},\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def respond(self, user_input: str, temperature: float = 0.6) -> str:\n",
        "        attempts = 0\n",
        "        while attempts < len(self.model_candidates):\n",
        "            try:\n",
        "                config = GenerationConfig(temperature=temperature)\n",
        "                response = self._chat.send_message(user_input, generation_config=config)\n",
        "                tool_call = self._next_tool_call(response)\n",
        "                while tool_call:\n",
        "                    tool_output = self._invoke_tool(tool_call[\"name\"], tool_call[\"args\"])\n",
        "                    response = self._chat.send_message(tool_output, generation_config=config)\n",
        "                    tool_call = self._next_tool_call(response)\n",
        "                return self._extract_text(response)\n",
        "            except (\n",
        "                google_exceptions.NotFound,\n",
        "                google_exceptions.FailedPrecondition,\n",
        "                google_exceptions.PermissionDenied,\n",
        "            ) as exc:\n",
        "                attempts += 1\n",
        "                if attempts >= len(self.model_candidates):\n",
        "                    raise RuntimeError(\n",
        "                        \"All configured Gemini models returned errors during respond(). \"\n",
        "                        f\"Models tried: {', '.join(self.model_candidates)}\"\n",
        "                    ) from exc\n",
        "                print(f\"{exc.__class__.__name__}: {exc} -- trying next model...\")\n",
        "                self._rotate_model()\n",
        "\n",
        "        raise RuntimeError(\"Unable to obtain a response from any Gemini model.\")\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self._chat = self._model.start_chat(history=[])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a4ef9f4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ef9f4e",
        "outputId": "157335af-c00a-4668-a07a-eb794b2eeaaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System prompt characters: 1479\n",
            "Using Gemini model: models/gemini-2.5-flash\n",
            "Active Gemini model: models/gemini-2.5-flash\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = build_system_prompt(summary_text, profile_snippet)\n",
        "print(f\"System prompt characters: {len(SYSTEM_PROMPT)}\")\n",
        "agent = GiftMuseAgent(api_key=GEMINI_API_KEY, model_candidates=MODEL_CANDIDATES, system_prompt=SYSTEM_PROMPT)\n",
        "print(\"Active Gemini model:\", agent.model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5befa1c1",
      "metadata": {
        "id": "5befa1c1"
      },
      "source": [
        "## Try the Concierge\n",
        "Uncomment the cells below to sample the assistant and trigger tool logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e51d37a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "e51d37a9",
        "outputId": "820737a3-8ea6-4733-b46e-4b46ea70c81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a joyous occasion, a sunset engagement! We'd be absolutely delighted to help you find a dazzling keepsake that captures the magic of such a moment.\n",
            "\n",
            "For a sunset engagement, how about a custom-engraved sundial that, instead of telling time, marks the exact latitude and longitude of their proposal spot? Or perhaps a beautifully crafted piece of jewelry from one of our artisan partners featuring a gemstone that mimics the warm hues of a sunset, like a fire opal or a morganite! Each piece tells a unique story and supports incredible craftsmanship.\n",
            "\n",
            "We're all about blending heartfelt curation with a touch of brilliance to make your gifting truly special. And, in line with our sustainability pledge, a portion of every concierge package supports creativity grants!\n",
            "\n",
            "To help us craft even more tailor-made ideas for you, would you like to share your contact details? We can then dive deeper into your preferences and ensure every detail is perfect!\n"
          ]
        }
      ],
      "source": [
        "agent.reset()\n",
        "reply = agent.respond(\"We are hosting a sunset engagement; can you suggest a dazzling keepsake?\")\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5d4fe94b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "5d4fe94b",
        "outputId": "0909448c-f0e3-4666-e3dc-7037e4b18f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for entrusting GiftMuse Atelier with this special project! Our concierge team will be reaching out to you very soon to discuss all the delightful details and brainstorm some truly unique, sustainable options for your partners. We're so excited to help you make a wonderful impression!\n"
          ]
        }
      ],
      "source": [
        "reply = agent.respond(\"Capture my info: Layla Haddad, layla@example.com, 25 eco gift boxes for our partners.\")\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.reset()\n",
        "reply = agent.respond(\"I need a gift for my 11 years sister's birthday, what do you suggest\")\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "RAsNU_k0vZTP",
        "outputId": "95372fe7-f440-4577-e47f-6160041c17f0"
      },
      "id": "RAsNU_k0vZTP",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, what a joy to help celebrate your sister's 11th birthday! Finding that perfect, heartfelt gift is our specialty here at GiftMuse Atelier. We're brimming with ideas to make her day truly special!\n",
            "\n",
            "For an 11-year-old, we often find that gifts that spark creativity, encourage discovery, or offer a touch of personalized magic are a huge hit. How about exploring these paths:\n",
            "\n",
            "1.  **Artisan Craft Kits:** Imagine a beautiful kit for making her own jewelry, learning calligraphy, or even a beginner's pottery set from one of our talented artisan partners. These are wonderful for encouraging new hobbies and creating something unique!\n",
            "2.  **Sustainable Storytelling & Reading:** A curated selection of engaging books from independent authors or publishers who align with our sustainability pledge. We could even pair it with a cozy, eco-friendly reading nook accessory.\n",
            "3.  **Personalized Keepsakes:** A custom-illustrated piece of art featuring her favorite animal or hobby, or a beautifully engraved item that celebrates her individuality. Our makers create truly special, lasting memories.\n",
            "\n",
            "We pride ourselves on blending warm human curation with agile AI research to pinpoint gifts that feel tailor-made. Plus, every concierge package supports creativity grants through our sustainability pledge!\n",
            "\n",
            "To help us narrow down the *perfect* suggestion, could you share a little more about her interests, hobbies, or what makes her light up? If you'd like us to scout some bespoke ideas just for her, please feel free to share your contact details!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd4c62c2",
      "metadata": {
        "id": "fd4c62c2"
      },
      "source": [
        "## Launch Gradio Chat Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fd4d0534",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "fd4d0534",
        "outputId": "f1a99543-f004-43b1-e754-d9e1a07ba663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://cd9eafbc0cc09d732a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cd9eafbc0cc09d732a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://cd9eafbc0cc09d732a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "gift_theme = gr.themes.Soft(\n",
        "    primary_hue=\"pink\",\n",
        "    secondary_hue=\"rose\",\n",
        "    neutral_hue=\"gray\",\n",
        "    font=(\"Poppins\", \"sans-serif\"),\n",
        ")\n",
        "\n",
        "GIFT_CSS = \"\"\"\n",
        ".gradio-container {background: linear-gradient(135deg, #fff5f7 0%, #ffeefc 100%);}\n",
        ".chatbot .message.user {background: rgba(255, 182, 193, 0.25); border-radius: 18px;}\n",
        ".chatbot .message.bot {background: rgba(255, 255, 255, 0.95); border-radius: 18px; border: 1px solid #ffd4e5;}\n",
        ".gradio-container .title {font-weight: 700; letter-spacing: 0.02em;}\n",
        ".gradio-container button {background: #ff5c8a !important; border-color: #ff5c8a !important;}\n",
        "\"\"\"\n",
        "\n",
        "def gradio_respond(message, history):\n",
        "    if not history:\n",
        "        agent.reset()\n",
        "    return agent.respond(message)\n",
        "\n",
        "chat = gr.ChatInterface(\n",
        "    fn=gradio_respond,\n",
        "    type=\"messages\",\n",
        "    title=\"GiftMuse Atelier Concierge\",\n",
        "    description=\"Share the occasion, vibe, and budget - I will suggest handcrafted gifting paths and tee up our concierge team.\",\n",
        "    theme=gift_theme,\n",
        "    css=GIFT_CSS,\n",
        "    examples=[\n",
        "        \"I need a wow-worthy gift for a 10-year work anniversary celebration.\",\n",
        "        \"We are planning 30 sustainable welcome kits - can you recommend artisan makers?\",\n",
        "        \"Do you offer last-minute curation and gift wrapping for Dubai deliveries?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "chat.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abcaebc8",
      "metadata": {
        "id": "abcaebc8"
      },
      "source": [
        "## Review Logged Leads and Feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3266541b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3266541b",
        "outputId": "26971799-1f0e-4673-d76d-1358711530e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "leads.jsonl:\n",
            "{'type': 'lead', 'timestamp': '2025-10-20T08:27:12.814255+00:00', 'name': 'Layla Haddad', 'email': 'layla@example.com', 'message': '25 eco gift boxes for our partners.'}\n",
            "{'type': 'lead', 'timestamp': '2025-10-20T08:34:17.274123+00:00', 'name': '[Your Name]', 'email': '[your email]', 'message': 'Gift for 11-year-old sister'}\n",
            "{'type': 'lead', 'timestamp': '2025-10-20T08:38:13.747378+00:00', 'name': 'Layla Haddad', 'email': 'layla@example.com', 'message': '25 eco gift boxes for our partners.'}\n",
            "{'type': 'lead', 'timestamp': '2025-10-20T08:48:11.092562+00:00', 'name': 'Layla Haddad', 'email': 'layla@example.com', 'message': '25 eco gift boxes for partners.'}\n",
            "\n",
            "feedback.jsonl:\n",
            "No entries yet.\n"
          ]
        }
      ],
      "source": [
        "for log_path in [LEADS_LOG, FEEDBACK_LOG]:\n",
        "    print(f\"\\n{log_path.name}:\")\n",
        "    if log_path.exists() and log_path.read_text().strip():\n",
        "        for line in log_path.read_text(encoding=\"utf-8\").splitlines():\n",
        "            print(json.loads(line))\n",
        "    else:\n",
        "        print(\"No entries yet.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}